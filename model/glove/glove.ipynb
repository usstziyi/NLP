{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341e48a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 传统的 Skip-gram 模型基于局部上下文窗口预测中心词的上下文词，其目标是最大化给定中心词ωi时，上下文词ωj出现的概率。\n",
    "# 然而，这种局部采样方式忽略了整个语料库中词之间的全局共现信息。\n",
    "# 为了更好地学习词向量，GloVe 模型引入了全局共现统计，即利用整个语料库中所有词对的共现频率（即全局统计），\n",
    "# 对损失函数进行重新定义，从而更高效地学习词向量。\n",
    "# 带全局语料统计的跳元模型 是一种将 Skip-gram 模型从“局部采样”升级为“全局共现建模”的方法，\n",
    "# 它通过使用整个语料库中词对的共现频率xij来构造损失函数，\n",
    "# 使得词向量学习更加稳定、高效，并能更好地捕捉词语间的语义关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabb252",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "讲解一下glove全局向量的词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812ca0c",
   "metadata": {},
   "source": [
    "当然可以！我们来一步一步详细解释这篇关于 **GloVe（Global Vectors for Word Representation）** 的内容。我会尽量用通俗易懂的语言，结合例子，帮你理解 GloVe 的核心思想、它和 Word2Vec 的区别，以及为什么它要这样设计。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、背景：词嵌入（Word Embedding）是什么？\n",
    "\n",
    "在自然语言处理中，计算机不能直接理解“猫”“狗”这样的词，所以我们要把词变成**数字向量**（比如 [0.2, -1.3, 0.8, ...]），这个过程叫**词嵌入**。好的词向量应该满足：**语义相近的词，向量也相近**。比如“国王 - 男人 + 女人 ≈ 女王”。\n",
    "\n",
    "常见的词嵌入方法有：\n",
    "- **Word2Vec**（包括 Skip-gram 和 CBOW）\n",
    "- **GloVe**（本文重点）\n",
    "\n",
    "---\n",
    "\n",
    "## 二、Word2Vec 的局限性\n",
    "\n",
    "Word2Vec（比如 Skip-gram）是**基于局部上下文窗口**训练的：\n",
    "- 它每次只看一个词周围的几个词（比如前后5个词）。\n",
    "- 它试图预测：给定中心词 “冰”，上下文词是 “冷” 的概率有多大。\n",
    "\n",
    "但 Word2Vec **没有利用整个语料库的全局统计信息**。比如：\n",
    "- “冰” 和 “固体” 一共共现了 10000 次\n",
    "- “冰” 和 “气体” 只共现了 100 次\n",
    "\n",
    "这些**全局共现次数**（co-occurrence counts）其实包含了很强的语义信息，但 Word2Vec 没有直接用。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、GloVe 的核心思想：用全局共现统计来训练词向量\n",
    "\n",
    "GloVe 的全称是 **Global Vectors**，意思是它**利用整个语料库的全局统计信息**来学习词向量。\n",
    "\n",
    "### 1. 什么是“共现”？\n",
    "\n",
    "- 如果词 A 和词 B 经常出现在同一个上下文窗口里，就说它们“共现”。\n",
    "- 我们可以统计一个矩阵 **X**，其中 **xᵢⱼ** 表示词 i 和词 j 在整个语料库中共现了多少次。\n",
    "\n",
    "例如：\n",
    "- x<sub>冰, 固体</sub> = 10000\n",
    "- x<sub>冰, 气体</sub> = 100\n",
    "- x<sub>蒸汽, 气体</sub> = 9000\n",
    "\n",
    "这些数字是**提前统计好的**，不是在训练中动态计算的。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、GloVe 的关键洞察：看“共现概率的比值”\n",
    "\n",
    "GloVe 的作者发现：**单独看共现次数意义不大，但看“比值”能揭示语义关系**。\n",
    "\n",
    "举个经典例子（来自论文）：\n",
    "\n",
    "| 词 wₖ       | solid（固体） | gas（气体） | water（水） | fashion（时尚） |\n",
    "|------------|---------------|-------------|-------------|------------------|\n",
    "| P(wₖ \\| ice)   | 0.00019       | 0.000066    | 0.003       | 0.000017         |\n",
    "| P(wₖ \\| steam) | 0.000022      | 0.00078     | 0.0022      | 0.000018         |\n",
    "| 比值 P(ice)/P(steam) | **8.9**         | **0.085**     | 1.36        | 0.96             |\n",
    "\n",
    "分析：\n",
    "- “solid” 和 “ice” 更相关 → 比值 **远大于 1**\n",
    "- “gas” 和 “steam” 更相关 → 比值 **远小于 1**\n",
    "- “water” 和两者都相关 → 比值 ≈ 1\n",
    "- “fashion” 和两者都无关 → 比值 ≈ 1\n",
    "\n",
    "👉 **这个比值能反映语义关系！**\n",
    "\n",
    "所以 GloVe 的目标是：**让词向量能够预测这种比值**。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、如何用向量表示这个比值？\n",
    "\n",
    "我们希望找到一个函数 f，使得：\n",
    "\n",
    "$$\n",
    "f(\\mathbf{u}_j, \\mathbf{u}_k, \\mathbf{v}_i) \\approx \\frac{P(w_j | w_i)}{P(w_k | w_i)}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- **vᵢ** 是中心词 wᵢ 的向量\n",
    "- **uⱼ, uₖ** 是上下文词的向量\n",
    "\n",
    "为了简化，GloVe 假设这个函数可以写成：\n",
    "\n",
    "$$\n",
    "f = \\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)} = \\exp\\left( (\\mathbf{u}_j - \\mathbf{u}_k)^\\top \\mathbf{v}_i \\right)\n",
    "$$\n",
    "\n",
    "而我们知道：\n",
    "$$\n",
    "\\frac{P(w_j | w_i)}{P(w_k | w_i)} = \\frac{x_{ij}/x_i}{x_{ik}/x_i} = \\frac{x_{ij}}{x_{ik}}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "$$\n",
    "\\exp\\left( (\\mathbf{u}_j - \\mathbf{u}_k)^\\top \\mathbf{v}_i \\right) \\approx \\frac{x_{ij}}{x_{ik}}\n",
    "$$\n",
    "\n",
    "两边取对数：\n",
    "$$\n",
    "(\\mathbf{u}_j - \\mathbf{u}_k)^\\top \\mathbf{v}_i \\approx \\log x_{ij} - \\log x_{ik}\n",
    "$$\n",
    "\n",
    "整理后得到：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + \\text{常数}\n",
    "$$\n",
    "\n",
    "但这个“常数”其实和 i 或 j 有关（比如词频），所以 GloVe 引入了两个**偏置项**：\n",
    "- **bᵢ**：中心词 wᵢ 的偏置\n",
    "- **cⱼ**：上下文词 wⱼ 的偏置\n",
    "\n",
    "最终得到核心公式：\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log x_{ij}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 六、GloVe 的损失函数\n",
    "\n",
    "为了让上面的等式成立，GloVe 最小化以下**平方误差**：\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i,j} h(x_{ij}) \\left( \\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log x_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- **h(xᵢⱼ)** 是一个权重函数，用来处理稀疏性问题：\n",
    "  - 如果 xᵢⱼ = 0（没共现过），就跳过（因为 log(0) 无意义）\n",
    "  - 如果 xᵢⱼ 很小（比如 < 100），就给小权重，避免噪声干扰\n",
    "  - 通常：  \n",
    "    $$\n",
    "    h(x) = \n",
    "    \\begin{cases}\n",
    "    (x / c)^\\alpha & \\text{if } x < c \\\\\n",
    "    1 & \\text{if } x \\geq c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    （比如 c=100, α=0.75）\n",
    "\n",
    "---\n",
    "\n",
    "## 七、GloVe 和 Word2Vec 的区别总结\n",
    "\n",
    "| 特性 | Word2Vec (Skip-gram) | GloVe |\n",
    "|------|------------------------|-------|\n",
    "| 训练方式 | 局部窗口，预测上下文 | 全局共现统计 |\n",
    "| 损失函数 | 交叉熵（softmax） | 平方误差（回归） |\n",
    "| 是否对称 | 不对称（P(w_j\\|w_i) ≠ P(w_i\\|w_j)） | 对称（xᵢⱼ = xⱼᵢ） |\n",
    "| 是否用全局信息 | 否 | 是 |\n",
    "| 计算效率 | 训练快，但需负采样等技巧 | 需预计算共现矩阵，但训练稳定 |\n",
    "\n",
    "> 💡 实际使用中，GloVe 的词向量常常表现更好，尤其在小数据集上。\n",
    "\n",
    "---\n",
    "\n",
    "## 八、最后：词向量怎么用？\n",
    "\n",
    "训练完后，每个词有两个向量：\n",
    "- **vᵢ**（作为中心词）\n",
    "- **uᵢ**（作为上下文词）\n",
    "\n",
    "理论上它们应该一样，但因为初始化不同，实际略有差异。GloVe 的做法是：\n",
    "> **最终词向量 = vᵢ + uᵢ**\n",
    "\n",
    "这样能融合两种角色的信息。\n",
    "\n",
    "---\n",
    "\n",
    "## 九、举个简单例子 🌰\n",
    "\n",
    "假设我们有小语料：\n",
    "> “I like ice cream. Steam rises from hot water.”\n",
    "\n",
    "统计共现（窗口大小=2）：\n",
    "- “ice” 和 “solid” 共现 0 次（假设没出现）\n",
    "- 但如果我们有大语料，“ice” 和 “cold” 共现很多次\n",
    "\n",
    "GloVe 会发现：\n",
    "- log(x<sub>ice, cold</sub>) 很大\n",
    "- 所以会让 **v<sub>ice</sub> · u<sub>cold</sub>** 也很大 → 说明这两个向量方向接近\n",
    "\n",
    "最终，“ice” 和 “cold” 的向量就靠得很近！\n",
    "\n",
    "---\n",
    "\n",
    "## 十、总结一句话\n",
    "\n",
    "> **GloVe 通过拟合词与词之间的全局共现次数的对数，用简单的平方损失训练出高质量的词向量，既利用了统计规律，又避免了复杂概率归一化。**\n",
    "\n",
    "---\n",
    "\n",
    "如果你还有哪里不明白，比如数学推导、代码实现、或和 Word2Vec 的具体对比，欢迎继续问我！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368fa9d",
   "metadata": {},
   "source": [
    "这是一个非常好的问题！我们来**一步一步详细解释**这个推导过程，尤其是：\n",
    "\n",
    "> 从  \n",
    "> $$\n",
    "> (\\mathbf{u}_j - \\mathbf{u}_k)^\\top \\mathbf{v}_i \\approx \\log x_{ij} - \\log x_{ik}\n",
    "> $$  \n",
    "> 到  \n",
    "> $$\n",
    "> \\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + \\text{常数}\n",
    "> $$  \n",
    "> 是如何“整理”得到的。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 第一步：理解原始等式\n",
    "\n",
    "我们有：\n",
    "$$\n",
    "(\\mathbf{u}_j - \\mathbf{u}_k)^\\top \\mathbf{v}_i \\approx \\log x_{ij} - \\log x_{ik}\n",
    "$$\n",
    "\n",
    "展开左边：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i - \\mathbf{u}_k^\\top \\mathbf{v}_i \\approx \\log x_{ij} - \\log x_{ik}\n",
    "$$\n",
    "\n",
    "这表示：  \n",
    "**词 j 和 i 的向量点积** 减去 **词 k 和 i 的向量点积**，约等于它们共现次数对数的差。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 第二步：关键洞察 —— “常数”是什么？\n",
    "\n",
    "注意：这个等式是**对任意两个上下文词 j 和 k** 成立的。也就是说，对于同一个中心词 $ w_i $，无论你选哪个上下文词 $ w_j $ 或 $ w_k $，上面的关系都成立。\n",
    "\n",
    "我们可以把这个等式改写为：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + \\left( \\mathbf{u}_k^\\top \\mathbf{v}_i - \\log x_{ik} \\right)\n",
    "$$\n",
    "\n",
    "现在观察右边括号里的部分：\n",
    "- $ \\mathbf{u}_k^\\top \\mathbf{v}_i $ 是一个向量点积（依赖于 k）\n",
    "- $ \\log x_{ik} $ 是共现对数（也依赖于 k）\n",
    "\n",
    "但如果我们固定中心词 $ w_i $，那么对于所有不同的上下文词 $ w_k $，这个括号中的值**可能不是完全相同的**。\n",
    "\n",
    "但是！GloVe 做了一个**关键假设**：\n",
    "\n",
    "> 对于固定的中心词 $ w_i $，表达式 $ \\mathbf{u}_k^\\top \\mathbf{v}_i - \\log x_{ik} $ 在不同 k 上变化不大，可以近似看作一个**与 j 无关的常数**。\n",
    "\n",
    "换句话说，虽然它依赖于 k，但它是**关于 j 的“背景项”**，所以当我们只关心 $ \\mathbf{u}_j^\\top \\mathbf{v}_i $ 和 $ \\log x_{ij} $ 的关系时，可以把这部分当作“常数”处理。\n",
    "\n",
    "于是我们得到：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + C_i\n",
    "$$\n",
    "其中 $ C_i = \\mathbf{u}_k^\\top \\mathbf{v}_i - \\log x_{ik} $ 是一个**只与中心词 i 有关的常数**。\n",
    "\n",
    "> ✅ 这就是为什么说：“整理后得到：$ \\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + \\text{常数} $”\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 举个例子说明\n",
    "\n",
    "假设我们有一个中心词 “ice”，它的向量是 $ \\mathbf{v}_{\\text{ice}} $\n",
    "\n",
    "考虑两个上下文词：\n",
    "1. “cold” → 共现次数 $ x_{\\text{ice,cold}} = 1000 $\n",
    "2. “hot” → 共现次数 $ x_{\\text{ice,hot}} = 10 $\n",
    "\n",
    "根据前面的公式：\n",
    "$$\n",
    "(\\mathbf{u}_{\\text{cold}} - \\mathbf{u}_{\\text{hot}})^\\top \\mathbf{v}_{\\text{ice}} \\approx \\log 1000 - \\log 10 = 6.91 - 2.30 = 4.61\n",
    "$$\n",
    "\n",
    "即：\n",
    "$$\n",
    "\\mathbf{u}_{\\text{cold}}^\\top \\mathbf{v}_{\\text{ice}} - \\mathbf{u}_{\\text{hot}}^\\top \\mathbf{v}_{\\text{ice}} \\approx 4.61\n",
    "$$\n",
    "\n",
    "如果我们把 $ \\mathbf{u}_{\\text{hot}}^\\top \\mathbf{v}_{\\text{ice}} $ 看作某个“基准值”，比如设它为 $ c $，那么：\n",
    "$$\n",
    "\\mathbf{u}_{\\text{cold}}^\\top \\mathbf{v}_{\\text{ice}} \\approx \\log 1000 + c = 6.91 + c\n",
    "$$\n",
    "\n",
    "同理：\n",
    "$$\n",
    "\\mathbf{u}_{\\text{water}}^\\top \\mathbf{v}_{\\text{ice}} \\approx \\log x_{\\text{ice,water}} + c\n",
    "$$\n",
    "\n",
    "👉 所以，对于同一个中心词 “ice”，不管上下文词是什么，都有：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_{\\text{ice}} \\approx \\log x_{\\text{ice,j}} + c\n",
    "$$\n",
    "\n",
    "这里的 $ c $ 就是那个“常数”。\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 注意：这不是严格数学恒等式，而是模型假设\n",
    "\n",
    "实际上，这个“常数”并不总是完全不变的，但它是一个**合理的近似**。为了更精确地建模，GloVe 引入了偏置项：\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log x_{ij}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ b_i $：中心词 $ w_i $ 的偏置（对应上面的“常数”）\n",
    "- $ c_j $：上下文词 $ w_j $ 的偏置（用于修正不同上下文词的影响）\n",
    "\n",
    "这样就不再需要强行认为“常数”是固定的，而是用参数显式建模。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：为什么能“整理”成那样？\n",
    "\n",
    "| 步骤 | 解释 |\n",
    "|------|------|\n",
    "| 1. 从比值出发 | $ \\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)} \\approx \\frac{x_{ij}}{x_{ik}} $ |\n",
    "| 2. 取对数 | $ \\mathbf{u}_j^\\top \\mathbf{v}_i - \\mathbf{u}_k^\\top \\mathbf{v}_i \\approx \\log x_{ij} - \\log x_{ik} $ |\n",
    "| 3. 移项 | $ \\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + (\\mathbf{u}_k^\\top \\mathbf{v}_i - \\log x_{ik}) $ |\n",
    "| 4. 关键假设 | $ \\mathbf{u}_k^\\top \\mathbf{v}_i - \\log x_{ik} $ 对于固定 $ i $ 是一个常数（或可视为常数） |\n",
    "| 5. 得到结论 | $ \\mathbf{u}_j^\\top \\mathbf{v}_i \\approx \\log x_{ij} + \\text{常数} $ |\n",
    "\n",
    "> 🎯 所以，“整理”并不是严格的代数变形，而是在**合理假设下进行的简化**，目的是让模型更容易训练和解释。\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想看这个公式的完整推导过程，或者想了解怎么用 Python 实现 GloVe，也可以继续问我！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353bbbd",
   "metadata": {},
   "source": [
    "这是一个非常关键的问题！\n",
    "\n",
    "我们来详细解释：  \n",
    "> **为什么 GloVe 要假设这个函数可以写成**  \n",
    "> $$\n",
    "> f = \\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)}\n",
    "> $$\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 核心目标：用向量预测“共现概率的比值”\n",
    "\n",
    "GloVe 的核心思想是：\n",
    "\n",
    "> 我们不直接预测某个词出现的概率，而是预测 **两个词相对于同一个中心词的共现概率之比**。\n",
    "\n",
    "比如：\n",
    "- 给定中心词 “ice”，它和 “solid” 共现很多次\n",
    "- 它和 “gas” 共现很少\n",
    "- 所以：  \n",
    "  $$\n",
    "  \\frac{P(\\text{solid} \\mid \\text{ice})}{P(\\text{gas} \\mid \\text{ice})} \\gg 1\n",
    "  $$\n",
    "\n",
    "我们希望模型能学到这种关系。也就是说，我们要找一个函数 $ f $，使得：\n",
    "$$\n",
    "f(\\text{solid}, \\text{gas}, \\text{ice}) \\approx \\frac{P(\\text{solid} \\mid \\text{ice})}{P(\\text{gas} \\mid \\text{ice})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 为什么选择这个形式？\n",
    "\n",
    "### 1. 这个形式来自 Word2Vec 的 Skip-gram 模型\n",
    "\n",
    "在 Word2Vec 中，Skip-gram 模型使用 softmax 来预测上下文词的概率：\n",
    "\n",
    "$$\n",
    "P(w_j \\mid w_i) = \\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\sum_{k=1}^V \\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "$$\n",
    "\\frac{P(w_j \\mid w_i)}{P(w_k \\mid w_i)} = \\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)}\n",
    "$$\n",
    "\n",
    "👉 看！这正是 GloVe 假设的形式！\n",
    "\n",
    "所以 GloVe 并不是凭空捏造这个公式，而是**借鉴了 Word2Vec 的成功经验**，但做了简化：**只关注比值，不关心归一化**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 为什么要用指数形式？（为什么是 exp？）\n",
    "\n",
    "指数函数有两个重要性质：\n",
    "\n",
    "#### ✅ (1) 非负性\n",
    "- $ \\exp(x) \\geq 0 $\n",
    "- 所以 $ f = \\frac{\\exp(a)}{\\exp(b)} > 0 $\n",
    "- 概率比必须是正数，符合要求\n",
    "\n",
    "#### ✅ (2) 对数后线性化\n",
    "- 取对数：  \n",
    "  $$\n",
    "  \\log f = \\mathbf{u}_j^\\top \\mathbf{v}_i - \\mathbf{u}_k^\\top \\mathbf{v}_i\n",
    "  $$\n",
    "- 这变成了一个**线性组合**，便于优化！\n",
    "\n",
    "这是最关键的一点：  \n",
    "> 使用指数形式，可以让对数后的表达式变成简单的向量点积差，从而让整个问题变得**可训练、可优化**。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 为什么不用别的形式？比如多项式？\n",
    "\n",
    "假设我们尝试用其他函数，比如：\n",
    "$$\n",
    "f = \\mathbf{u}_j^\\top \\mathbf{v}_i - \\mathbf{u}_k^\\top \\mathbf{v}_i\n",
    "$$\n",
    "\n",
    "虽然看起来简单，但它可能为负，而概率比总是正的。\n",
    "\n",
    "或者：\n",
    "$$\n",
    "f = \\frac{\\mathbf{u}_j^\\top \\mathbf{v}_i}{\\mathbf{u}_k^\\top \\mathbf{v}_i}\n",
    "$$\n",
    "\n",
    "这可能会导致除零或不稳定。\n",
    "\n",
    "而指数形式：\n",
    "- 自动保证正值\n",
    "- 对数后线性化\n",
    "- 数学上优雅且稳定\n",
    "\n",
    "所以它是**最优选择之一**。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 举个例子说明\n",
    "\n",
    "假设我们有：\n",
    "- $ \\mathbf{u}_{\\text{cold}}^\\top \\mathbf{v}_{\\text{ice}} = 5 $\n",
    "- $ \\mathbf{u}_{\\text{hot}}^\\top \\mathbf{v}_{\\text{ice}} = 1 $\n",
    "\n",
    "那么：\n",
    "$$\n",
    "f = \\frac{\\exp(5)}{\\exp(1)} = e^{4} \\approx 54.6\n",
    "$$\n",
    "\n",
    "这意味着：“cold” 相对于 “hot” 出现在 “ice” 附近的概率高了约 55 倍 —— 符合常识！\n",
    "\n",
    "如果换成普通点积：\n",
    "$$\n",
    "\\frac{5}{1} = 5\n",
    "$$\n",
    "也合理，但无法捕捉到指数级的差异。\n",
    "\n",
    "而指数形式能更好地反映**语义强度的差异**。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：为什么这样假设？\n",
    "\n",
    "| 原因 | 解释 |\n",
    "|------|------|\n",
    "| 🔹 来源于 Word2Vec | 是 Skip-gram 概率比的自然形式 |\n",
    "| 🔹 保证正数 | 指数恒为正，适合表示概率比 |\n",
    "| 🔹 对数后线性化 | 训练时可以用梯度下降轻松优化 |\n",
    "| 🔹 数学简洁 | 表达式简单，易于实现和理解 |\n",
    "| 🔹 实验有效 | 在实际中表现很好，优于其他形式 |\n",
    "\n",
    "> 💡 所以，这不是随便假设的，而是基于已有模型的成功经验 + 数学上的合理性 + 实际效果验证的结果。\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想看如何从这个假设推导出最终的损失函数，我也可以继续讲！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f853b6",
   "metadata": {},
   "source": [
    "这是一个非常关键的问题！\n",
    "\n",
    "我们来一步一步详细解释：\n",
    "\n",
    "> **为什么 GloVe 要最小化这个平方误差？**\n",
    ">\n",
    "> $$\n",
    "> \\text{Loss} = \\sum_{i,j} h(x_{ij}) \\left( \\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log x_{ij} \\right)^2\n",
    "> $$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 回顾：GloVe 的核心目标\n",
    "\n",
    "GloVe 的目标是让词向量满足以下关系：\n",
    "$$\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log x_{ij}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ x_{ij} $：词 $ i $ 和词 $ j $ 在整个语料库中的**共现次数**\n",
    "- $ \\mathbf{u}_j $：上下文词 $ j $ 的向量（用于预测）\n",
    "- $ \\mathbf{v}_i $：中心词 $ i $ 的向量（被预测的词）\n",
    "- $ b_i, c_j $：偏置项，用来修正词频等偏差\n",
    "\n",
    "👉 所以，我们的任务就是：**找到一组向量和偏置，使得这个近似尽可能准确。**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 为什么要用“平方误差”？\n",
    "\n",
    "### ✅ 1. 最小二乘法是最经典的回归方法\n",
    "\n",
    "我们要拟合的是：\n",
    "$$\n",
    "\\text{预测值} = \\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j\n",
    "$$\n",
    "$$\n",
    "\\text{真实值} = \\log x_{ij}\n",
    "$$\n",
    "\n",
    "这实际上是一个**回归问题**：用线性模型（点积+偏置）去拟合对数共现次数。\n",
    "\n",
    "而**最小化平方误差**是解决这类问题的标准方法，因为它：\n",
    "- 数学上容易求导\n",
    "- 有解析解或高效优化算法\n",
    "- 对异常值有一定鲁棒性（虽然不如 Huber 损失）\n",
    "\n",
    "所以选择平方误差是自然且合理的。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 2. 平方误差能引导模型“逼近真实值”\n",
    "\n",
    "考虑一个简单的例子：\n",
    "\n",
    "| 共现次数 | log(x) | 预测值 | 误差 |\n",
    "|---------|--------|--------|------|\n",
    "| 100     | 4.6    | 5.0    | (5.0 - 4.6)² = 0.16 |\n",
    "| 1       | 0.0    | 0.5    | (0.5 - 0.0)² = 0.25 |\n",
    "\n",
    "如果预测值离真实值越远，损失越大 → 模型会被“惩罚”，从而调整参数。\n",
    "\n",
    "所以通过最小化这个平方差，模型会不断调整 $ \\mathbf{u}_j, \\mathbf{v}_i, b_i, c_j $，使预测更接近真实值。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 为什么加权重函数 $ h(x_{ij}) $？\n",
    "\n",
    "注意：不是所有共现都一样重要！\n",
    "\n",
    "比如：\n",
    "- $ x_{ij} = 10000 $：经常一起出现 → 很重要\n",
    "- $ x_{ij} = 1 $：只出现一次 → 可能是噪声\n",
    "- $ x_{ij} = 0 $：从不共现 → 忽略\n",
    "\n",
    "但如果我们直接对所有 $ x_{ij} $ 都计算误差，那稀疏的、偶然的共现会干扰训练。\n",
    "\n",
    "所以 GloVe 引入了权重函数：\n",
    "$$\n",
    "h(x_{ij}) = \n",
    "\\begin{cases}\n",
    "(x_{ij}/c)^\\alpha & \\text{if } x_{ij} < c \\\\\n",
    "1 & \\text{if } x_{ij} \\geq c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "常见参数：$ c = 100 $, $ \\alpha = 0.75 $\n",
    "\n",
    "### 举个例子：\n",
    "\n",
    "| xᵢⱼ | h(xᵢⱼ) | 说明 |\n",
    "|-----|--------|------|\n",
    "| 1   | (1/100)^0.75 ≈ 0.03 | 权重很小，忽略 |\n",
    "| 50  | (50/100)^0.75 ≈ 0.6 | 中等权重 |\n",
    "| 100 | 1      | 全权重 |\n",
    "| 1000| 1      | 全权重 |\n",
    "\n",
    "👉 这样做可以：\n",
    "- **避免稀疏共现带来的噪声**\n",
    "- **聚焦于频繁共现的词对**\n",
    "- **提高训练稳定性**\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 为什么不用其他损失函数？比如交叉熵？\n",
    "\n",
    "你可能会问：“既然 Word2Vec 用交叉熵，为什么 GloVe 不用？”\n",
    "\n",
    "原因如下：\n",
    "\n",
    "| 方法 | 优点 | 缺点 |\n",
    "|------|------|------|\n",
    "| **交叉熵**（Word2Vec） | 直接建模概率分布 | 计算复杂（需要归一化），训练慢 |\n",
    "| **平方误差**（GloVe） | 简单、快速、可并行 | 不直接建模概率，但效果很好 |\n",
    "\n",
    "GloVe 的优势在于：\n",
    "- 它不需要 softmax 归一化（省去了大量计算）\n",
    "- 它基于全局统计，不依赖局部窗口\n",
    "- 它的损失函数更容易优化\n",
    "\n",
    "所以在实践中，GloVe 的训练速度更快，尤其适合大规模语料。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：为什么最小化这个损失？\n",
    "\n",
    "| 原因 | 解释 |\n",
    "|------|------|\n",
    "| 🔹 目标是拟合 $ \\log x_{ij} $ | 我们希望向量点积加上偏置能逼近共现对数 |\n",
    "| 🔹 使用平方误差 | 是标准的回归方式，数学上简单且有效 |\n",
    "| 🔹 加上 $ h(x_{ij}) $ | 降低稀疏共现的影响，防止噪声干扰 |\n",
    "| 🔹 优化目标清晰 | 让模型学习到有意义的词向量，捕捉语义关系 |\n",
    "\n",
    "> 🌟 所以，这个损失函数的设计是**理论合理 + 实践高效**的结果。\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想看这个损失函数怎么用梯度下降优化，或者如何实现它，我也可以继续讲！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d479bbb",
   "metadata": {},
   "source": [
    "这是一个非常棒的问题！\n",
    "\n",
    "你问的是：\n",
    "\n",
    "> **为什么 GloVe 的损失函数里没有 $ \\frac{1}{N} $？**\n",
    ">\n",
    "> 比如我们平时学的均方误差（MSE）是：\n",
    "> $$\n",
    "> \\text{Loss} = \\frac{1}{N} \\sum_{i,j} (\\hat{y}_{ij} - y_{ij})^2\n",
    "> $$\n",
    "> 但 GloVe 写的是：\n",
    "> $$\n",
    "> \\text{Loss} = \\sum_{i,j} h(x_{ij}) \\left( \\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log x_{ij} \\right)^2\n",
    "> $$\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 答案一句话总结：\n",
    "\n",
    "> **因为 $ \\frac{1}{N} $ 只影响优化过程中的学习率，不影响最终模型的最优解。在实践中，它通常被省略，由优化器（如 SGD）自动处理。**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 我们来一步一步解释\n",
    "\n",
    "### 第一步：$ \\frac{1}{N} $ 是干什么的？\n",
    "\n",
    "在机器学习中，我们经常用 **均方误差（MSE）** 来衡量预测值和真实值之间的差距：\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ N $：样本总数\n",
    "- $ \\frac{1}{N} $：**归一化因子**，让损失不随数据量增大而变大\n",
    "\n",
    "👉 举个例子：\n",
    "- 如果有 1000 个词对，总误差是 1000\n",
    "- 如果有 10000 个词对，总误差是 10000\n",
    "- 加上 $ \\frac{1}{N} $ 后，两者都是平均误差 1 → 更容易比较\n",
    "\n",
    "---\n",
    "\n",
    "### 第二步：GloVe 为什么可以不用 $ \\frac{1}{N} $？\n",
    "\n",
    "#### ✅ 原因 1：**它只影响梯度大小，不影响最优解**\n",
    "\n",
    "假设我们有两个损失函数：\n",
    "\n",
    "1. $ L_1 = \\sum_{i,j} h(x_{ij}) (f_{ij} - \\log x_{ij})^2 $\n",
    "2. $ L_2 = \\frac{1}{N} \\sum_{i,j} h(x_{ij}) (f_{ij} - \\log x_{ij})^2 $\n",
    "\n",
    "它们的**最小值位置是一样的**！  \n",
    "也就是说：  \n",
    "> **无论有没有 $ \\frac{1}{N} $，模型都会学到相同的词向量。**\n",
    "\n",
    "只是：\n",
    "- $ L_1 $ 的损失值更大\n",
    "- 但梯度也更大 → 学习率要调小一点\n",
    "\n",
    "所以，**从数学角度看，$ \\frac{1}{N} $ 是可选的**。\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ 原因 2：**GloVe 使用的是随机梯度下降（SGD），不需要全局归一化**\n",
    "\n",
    "在训练 GloVe 时，通常是：\n",
    "- 随机采样一些词对 $ (i,j) $\n",
    "- 计算当前样本的梯度\n",
    "- 更新参数\n",
    "\n",
    "在这种情况下：\n",
    "- 每次更新只看一个或一小批样本\n",
    "- 不需要知道整个语料库的总数量 $ N $\n",
    "- 所以直接用 $ \\sum $ 就够了\n",
    "\n",
    "> 💡 这就像你在做线性回归时，可以用 `loss = sum((y - pred)**2)`，然后手动调整学习率，而不是一定要写 `mean_squared_error`。\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ 原因 3：**权重函数 $ h(x_{ij}) $ 已经做了“加权”处理**\n",
    "\n",
    "注意：GloVe 的损失函数是：\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i,j} h(x_{ij}) \\left( \\cdots \\right)^2\n",
    "$$\n",
    "\n",
    "其中 $ h(x_{ij}) $ 是一个权重函数，比如：\n",
    "- 当 $ x_{ij} < 100 $，$ h(x_{ij}) = (x_{ij}/100)^{0.75} $\n",
    "- 当 $ x_{ij} \\geq 100 $，$ h(x_{ij}) = 1 $\n",
    "\n",
    "这相当于说：\n",
    "- 高频共现词对权重高\n",
    "- 低频共现词对权重低\n",
    "\n",
    "👉 所以，这个加权机制已经起到了“平衡不同词对贡献”的作用，**不需要再额外除以 $ N $**。\n",
    "\n",
    "---\n",
    "\n",
    "### 第三步：实际中是否真的不用 $ \\frac{1}{N} $？\n",
    "\n",
    "✅ **是的，很多实现都不加。**\n",
    "\n",
    "例如，在原始论文《GloVe: Global Vectors for Word Representation》中，损失函数就是写的：\n",
    "$$\n",
    "J = \\sum_{i,j} h(x_{ij}) \\left( \\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log x_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "而且在代码实现中（如 Stanford 的 GloVe 代码），也没有显式除以 $ N $。\n",
    "\n",
    "但是！\n",
    "- 如果你要比较不同规模语料的损失值，最好加上 $ \\frac{1}{N} $\n",
    "- 或者用平均损失（如 `total_loss / num_samples`）\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：为什么没有 $ \\frac{1}{N} $？\n",
    "\n",
    "| 原因 | 解释 |\n",
    "|------|------|\n",
    "| 🔹 数学上不影响最优解 | 有无 $ \\frac{1}{N} $，模型学到的向量一样 |\n",
    "| 🔹 实践中用 SGD | 每次只更新少量样本，不需要全局归一化 |\n",
    "| 🔹 权重函数已足够 | $ h(x_{ij}) $ 已经做了重要性加权 |\n",
    "| 🔹 为了简洁 | 省略常数因子，公式更干净 |\n",
    "\n",
    "> 🌟 所以，**不是“忘了”，而是“没必要”** —— 这是一个常见的简化做法，在深度学习中很普遍。\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想看如何在 PyTorch 中实现这个损失函数（包括要不要加 $ \\frac{1}{N} $），我也可以继续讲！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
