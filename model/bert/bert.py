# bert全称：Bidirectional Encoder Representations from Transformers 来自Transformers的双向编码器
# 作用：将输入的文本序列转换为向量表示，用于下游任务

# BERT 是一种基于 Transformer 架构的预训练语言模型，它在预训练时使用了两个主要任务：

# 掩码语言建模（Masked Language Modeling, MLM）
# 下一句预测（Next Sentence Prediction, NSP）